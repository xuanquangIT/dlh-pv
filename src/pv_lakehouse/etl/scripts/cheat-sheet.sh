#!/bin/bash
# Quick Cheat Sheet - Copy & Paste Commands

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘          PV Lakehouse - Quick Command Reference             â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

echo "ğŸ“¦ BRONZE LAYER - Data Ingestion"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facilities.py --mode incremental"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facility_timeseries.py --mode incremental"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facility_weather.py --mode incremental"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facility_air_quality.py --mode incremental"
echo ""

echo "âœ¨ SILVER LAYER - Data Transformation"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py facility_master --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py hourly_energy --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py hourly_weather --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py hourly_air_quality --mode full"
echo ""

echo "ğŸ† GOLD LAYER - Analytics & Data Mart"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "# Dimension Tables (Load these first)"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_date --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_time --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_facility --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_weather_condition --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_air_quality_category --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_equipment_status --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_performance_issue --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py dim_model_version --mode full"
echo ""
echo "# Fact Tables (Load after dimensions)"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py fact_kpi_performance --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py fact_weather_impact --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py fact_air_quality_impact --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py fact_solar_forecast --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/gold/run_loader.py fact_root_cause_analysis --mode full"
echo ""

echo "ğŸ”„ BACKFILL - Load Historical Data"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "# Bronze Backfill (NOTE: Timeseries uses UTC time, auto-converts to Brisbane timezone)"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facilities.py --mode backfill"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facility_timeseries.py --mode backfill --date-start 2025-10-01T00:00:00 --date-end 2025-10-22T23:59:59"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facility_weather.py --mode backfill --start 2025-10-01 --end 2025-10-22"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/bronze/load_facility_air_quality.py --mode backfill --start 2025-10-01 --end 2025-10-22"
echo ""
echo "# Silver Transform (Full mode processes all Bronze data)"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py facility_master --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py hourly_energy --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py hourly_weather --mode full"
echo "bash src/pv_lakehouse/etl/scripts/spark-submit.sh src/pv_lakehouse/etl/silver/cli.py hourly_air_quality --mode full"
echo ""

echo "ğŸ—‘ï¸  DELETE DATA - Cleanup"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "# Bronze Layer"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM bronze.raw_facility_timeseries;\""
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM bronze.raw_facility_weather;\""
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM bronze.raw_facility_air_quality;\""
echo ""
echo "# Silver Layer"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM silver.clean_hourly_energy;\""
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM silver.clean_hourly_weather;\""
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM silver.clean_hourly_air_quality;\""
echo ""
echo "# Gold Layer - Dimensions"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM gold.dim_date;\""
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM gold.dim_facility;\""
echo ""
echo "# Gold Layer - Facts"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM gold.fact_kpi_performance;\""
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM gold.fact_weather_impact;\""
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \"DELETE FROM gold.fact_air_quality_impact;\""
echo ""

echo "ğŸ“Š QUERY DATA - Check Results"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "# Bronze - Count rows and date ranges (all sources)"
echo "docker exec -it trino trino --execute \"SELECT 'timeseries' as source, MIN(interval_ts) as min_ts, MAX(interval_ts) as max_ts, COUNT(DISTINCT interval_ts) as unique_hours, COUNT(*) as total_rows FROM iceberg.bronze.raw_facility_timeseries UNION ALL SELECT 'weather', MIN(weather_timestamp), MAX(weather_timestamp), COUNT(DISTINCT weather_timestamp), COUNT(*) FROM iceberg.bronze.raw_facility_weather UNION ALL SELECT 'air_quality', MIN(air_timestamp), MAX(air_timestamp), COUNT(DISTINCT air_timestamp), COUNT(*) FROM iceberg.bronze.raw_facility_air_quality;\""
echo ""
echo "# Silver - Check hourly data per facility"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \\"
echo "  \"SELECT facility_code, COUNT(*) as row_count FROM silver.clean_hourly_energy GROUP BY facility_code;\""
echo ""
echo "# Silver - Check for duplicates (should be 1.0 for all)"
echo "docker exec -it trino trino --execute \"SELECT 'clean_hourly_energy' as table_name, COUNT(*) as total_rows, MAX(cnt) as max_dup, AVG(CAST(cnt AS DOUBLE)) as avg_dup FROM (SELECT facility_code, date_hour, COUNT(*) as cnt FROM iceberg.silver.clean_hourly_energy GROUP BY facility_code, date_hour) UNION ALL SELECT 'clean_hourly_weather', COUNT(*), MAX(cnt), AVG(CAST(cnt AS DOUBLE)) FROM (SELECT facility_code, date_hour, COUNT(*) as cnt FROM iceberg.silver.clean_hourly_weather GROUP BY facility_code, date_hour) UNION ALL SELECT 'clean_hourly_air_quality', COUNT(*), MAX(cnt), AVG(CAST(cnt AS DOUBLE)) FROM (SELECT facility_code, date_hour, COUNT(*) as cnt FROM iceberg.silver.clean_hourly_air_quality GROUP BY facility_code, date_hour);\""
echo ""
echo "# Gold - Dimensions summary"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \\"
echo "  \"SELECT 'dim_date' as table_name, COUNT(*) as row_count FROM gold.dim_date \\"
echo "   UNION ALL SELECT 'dim_time', COUNT(*) FROM gold.dim_time \\"
echo "   UNION ALL SELECT 'dim_facility', COUNT(*) FROM gold.dim_facility \\"
echo "   UNION ALL SELECT 'dim_weather_condition', COUNT(*) FROM gold.dim_weather_condition \\"
echo "   UNION ALL SELECT 'dim_air_quality_category', COUNT(*) FROM gold.dim_air_quality_category \\"
echo "   UNION ALL SELECT 'dim_equipment_status', COUNT(*) FROM gold.dim_equipment_status \\"
echo "   UNION ALL SELECT 'dim_performance_issue', COUNT(*) FROM gold.dim_performance_issue \\"
echo "   UNION ALL SELECT 'dim_model_version', COUNT(*) FROM gold.dim_model_version;\""
echo ""
echo "# Gold - Facts summary"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \\"
echo "  \"SELECT 'fact_kpi_performance' as table_name, COUNT(*) as row_count FROM gold.fact_kpi_performance \\"
echo "   UNION ALL SELECT 'fact_weather_impact', COUNT(*) FROM gold.fact_weather_impact \\"
echo "   UNION ALL SELECT 'fact_air_quality_impact', COUNT(*) FROM gold.fact_air_quality_impact \\"
echo "   UNION ALL SELECT 'fact_solar_forecast', COUNT(*) FROM gold.fact_solar_forecast \\"
echo "   UNION ALL SELECT 'fact_root_cause_analysis', COUNT(*) FROM gold.fact_root_cause_analysis;\""
echo ""
echo "# Gold - Check for duplicates in facts (should be 1.0 for all)"
echo "docker exec -it trino trino --execute \"SELECT 'fact_weather_impact' as table_name, COUNT(*) as total, MAX(cnt) as max_dup, AVG(CAST(cnt AS DOUBLE)) as avg_dup FROM (SELECT facility_key, date_key, time_key, COUNT(*) as cnt FROM iceberg.gold.fact_weather_impact GROUP BY facility_key, date_key, time_key) UNION ALL SELECT 'fact_air_quality_impact', COUNT(*), MAX(cnt), AVG(CAST(cnt AS DOUBLE)) FROM (SELECT facility_key, date_key, time_key, COUNT(*) as cnt FROM iceberg.gold.fact_air_quality_impact GROUP BY facility_key, date_key, time_key) UNION ALL SELECT 'fact_root_cause_analysis', COUNT(*), MAX(cnt), AVG(CAST(cnt AS DOUBLE)) FROM (SELECT facility_key, date_key, time_key, COUNT(*) as cnt FROM iceberg.gold.fact_root_cause_analysis GROUP BY facility_key, date_key, time_key) UNION ALL SELECT 'fact_solar_forecast', COUNT(*), MAX(cnt), AVG(CAST(cnt AS DOUBLE)) FROM (SELECT facility_key, date_key, time_key, COUNT(*) as cnt FROM iceberg.gold.fact_solar_forecast GROUP BY facility_key, date_key, time_key) UNION ALL SELECT 'fact_kpi_performance', COUNT(*), MAX(cnt), AVG(CAST(cnt AS DOUBLE)) FROM (SELECT facility_key, date_key, COUNT(*) as cnt FROM iceberg.gold.fact_kpi_performance GROUP BY facility_key, date_key);\""
echo ""
echo "# Gold - Sample KPI data"
echo "docker compose -f docker/docker-compose.yml exec trino trino --catalog iceberg --schema lh --execute \\"
echo "  \"SELECT facility_key, date_key, actual_energy_mwh, performance_ratio_pct \\"
echo "   FROM gold.fact_kpi_performance ORDER BY date_key DESC LIMIT 10;\""
echo ""

echo "ğŸ¤– MLFLOW - Machine Learning"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "# Run ML training (writes to fact_solar_forecast_ml table)"
echo "docker compose -f docker/docker-compose.yml exec spark-master bash -lc 'export AWS_ACCESS_KEY_ID=mlflow_svc AWS_SECRET_ACCESS_KEY=pvlakehouse_mlflow MLFLOW_S3_ENDPOINT_URL=http://minio:9000 AWS_REGION=us-east-1; cd /opt/workdir && python3 src/pv_lakehouse/mlflow/spark_ml_test.py --limit 500'"
echo ""
echo "# Access MLflow UI"
echo "echo \"MLflow UI: http://localhost:5000\""
echo ""

echo "ğŸ³ DOCKER - Service Management"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "docker compose -f docker/docker-compose.yml up -d"
echo "docker compose -f docker/docker-compose.yml ps"
echo "docker compose -f docker/docker-compose.yml logs -f spark-master"
echo "docker compose -f docker/docker-compose.yml exec spark-master bash"
echo ""

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âš¡ IMPORTANT NOTES"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "TIMEZONE:"
echo "  â€¢ Timeseries: Input UTC datetime (auto-converts to Brisbane UTC+10 for API)"
echo "  â€¢ Weather/Air Quality: Input date only (API uses UTC timezone)"
echo "  â€¢ Example: --date-start 2025-10-01T00:00:00 â†’ Data starts exactly at 2025-10-01 00:00 UTC"
echo ""
echo "GOLD LAYER DEPENDENCIES:"
echo "  â€¢ Dimensions MUST be loaded before Facts"
echo "  â€¢ Facts depend on: dim_facility, dim_time, dim_date, dim_weather_condition, etc."
echo "  â€¢ Order: Bronze â†’ Silver â†’ Gold Dimensions â†’ Gold Facts"
echo ""
echo "DATA QUALITY - NO DUPLICATES:"
echo "  â€¢ âœ… All Silver tables: Grain = (facility_code, date_hour)"
echo "  â€¢ âœ… All Gold hourly facts: Grain = (facility_key, date_key, time_key)"
echo "  â€¢ âœ… Gold daily facts: Grain = (facility_key, date_key)"
echo "  â€¢ âœ… Duplicate checks: max_dup=1.0, avg_dup=1.0 for all tables"
echo "  â€¢ Bug fixed: Removed daily aggregation in weather/air_quality lookups"
echo "  â€¢ Bug fixed: Added specific issue_type join in fact_root_cause_analysis"
echo ""
echo "PERFORMANCE OPTIMIZATIONS (Gold Layer):"
echo "  â€¢ âœ… Broadcast joins for small dimensions (5-10x faster)"
echo "  â€¢ âœ… Shuffle partitions optimized: 8 (vs 200 default)"
echo "  â€¢ âœ… Auto-broadcast threshold: 10MB"
echo "  â€¢ âœ… Silver chunking: 3-day (energy), 7-day (weather/air_quality)"
echo "  â€¢ Expected runtime: ~40-60 seconds for full Gold load"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ’¡ QUICK TIPS"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "  â€¢ All ETL scripts: src/pv_lakehouse/etl/{bronze,silver,gold}/"
echo "  â€¢ Spark submit wrapper: src/pv_lakehouse/etl/scripts/spark-submit.sh"
echo "  â€¢ Spark configs: src/pv_lakehouse/etl/utils/spark_utils.py"
echo "  â€¢ Documentation: doc/optimization/gold-layer-optimization.md"
echo "  â€¢ Run all layers: bash src/pv_lakehouse/etl/scripts/spark-submit.sh <script> --mode full"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
