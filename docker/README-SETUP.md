# Quick Start

## For New Team Members

**Get the entire lakehouse platform running in 3 commands:**

```bash
cd docker
docker compose --profile core --profile ml --profile orchestrate up -d --build
./scripts/health-check.sh
```

That's it!

## What Gets Set Up Automatically

- âœ… MinIO object storage with buckets: `lakehouse`, `mlflow`
- âœ… PostgreSQL with databases: `iceberg`, `iceberg_catalog`, `mlflow`, `prefect`
- âœ… **Apache Iceberg JDBC Catalog** (PostgreSQL-based, no Gravitino needed)
- âœ… Trino SQL query engine with Iceberg catalog configured
- âœ… Apache Spark cluster with S3A support
- âœ… MLflow experiment tracking
- âœ… Prefect workflow orchestration
- âœ… Service users with least-privilege bucket policies
- âœ… All networking and healthchecks configured
- âœ… Trino Iceberg schemas: `lh`

## Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| MinIO Console | http://localhost:9001 | pvlakehouse / pvlakehouse |
| Trino UI | http://localhost:8081 | - |
| MLflow UI | http://localhost:5000 | - |
| Prefect UI | http://localhost:4200 | - |
| Spark Master UI | http://localhost:4040 | - |

## Common Commands

**Start everything:**
```bash
docker compose --profile core --profile ml --profile orchestrate up -d
```

**Stop everything:**
```bash
docker compose down
```

**View logs:**
```bash
docker compose logs -f <service-name>
# Example: docker compose logs -f spark-master
```

**Restart a service:**
```bash
docker compose restart <service-name>
```

**Run health check:**
```bash
./scripts/health-check.sh
```

**Clean up (âš ï¸ removes all data):**
```bash
docker compose down -v
```

## Health Check Script

The `health-check.sh` script validates:

1. All containers are healthy
2. MinIO buckets exist
3. MinIO policies are created  
4. Service users exist with correct permissions
5. PostgreSQL databases exist
6. Iceberg catalog tables exist
7. Trino Iceberg catalog is configured correctly
   - SHOW CATALOGS lists `iceberg`
   - SHOW SCHEMAS FROM iceberg returns schemas
   - SELECT 1 query works
   - DDL permissions verified (schema creation)
   - Schema `lh` exists in Iceberg catalog
8. All service endpoints respond
9. Configuration files exist in repository
10. End-to-end S3A write test (Trino -> MinIO)

## Troubleshooting

**Containers not starting?**
```bash
docker compose logs <service-name>
docker compose ps
```

**Need to rebuild?**
```bash
docker compose down
docker compose up -d --build
```

**Port conflicts?**
```bash
# Check what's using a port
sudo netstat -tlnp | grep <port>
# Or on macOS:
lsof -i :<port>
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Lakehouse                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Storage   â”‚   Catalog    â”‚   Compute   â”‚     ML      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   MinIO     â”‚  PostgreSQL  â”‚    Trino    â”‚   MLflow    â”‚
â”‚  (S3 API)   â”‚ JDBC Catalog â”‚   (SQL)     â”‚ (Tracking)  â”‚
â”‚             â”‚  (Iceberg    â”‚             â”‚             â”‚
â”‚             â”‚  metadata)   â”‚    Spark    â”‚   Prefect   â”‚
â”‚             â”‚              â”‚  (Python)   â”‚ (Workflows) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Note:** We use Iceberg's JDBC catalog with PostgreSQL instead of REST catalog.
This eliminates Hadoop library compatibility issues and simplifies the architecture.

## Trino Iceberg Catalog

Trino is configured with an Iceberg catalog using **PostgreSQL JDBC catalog** with S3A storage on MinIO.

**Verify Trino Iceberg Catalog:**
```bash
# Connect to Trino CLI
docker exec -it trino trino

# In the Trino CLI, run:
SHOW CATALOGS;                    -- Should list: iceberg, system
SHOW SCHEMAS FROM iceberg;        -- Should list: information_schema, lh, system
SELECT 1;                          -- Basic query test
```

**Create tables in Iceberg:**
```sql
-- Create a test table
CREATE TABLE iceberg.lh.test_table (
    id INT,
    name VARCHAR
) WITH (
    format = 'PARQUET'
);

-- Insert data
INSERT INTO iceberg.lh.test_table VALUES (1, 'test');

-- Query data
SELECT * FROM iceberg.lh.test_table;

-- View metadata in PostgreSQL
-- Run from host:
docker exec postgres psql -U pvlakehouse -d iceberg_catalog -c "SELECT * FROM iceberg_tables;"
```

**Configuration Details:**
- Catalog type: `jdbc` (PostgreSQL JDBC)
- Database: `iceberg_catalog` in PostgreSQL
- Warehouse location: `s3a://lakehouse/warehouse`
- S3 endpoint: `http://minio:9000`
- Path-style access: `true`
- Service user: `trino_svc` with `lakehouse-rw` policy

**Metadata Tables:**
- `iceberg_tables` - Stores table locations and metadata
- `iceberg_namespace_properties` - Stores schema/namespace properties

**Note:** Schema tables are automatically created by `postgres-init.sql` on first startup.

Configuration file: `docker/trino/catalog/iceberg.properties`

**Benefits of JDBC Catalog:**
- âœ… No Hadoop version conflicts
- âœ… Simpler architecture (no REST catalog service needed)
- âœ… Direct PostgreSQL access for debugging
- âœ… Better performance (no REST overhead)
- âœ… Production-ready and stable

## What's Different from Standard Setup?

1. **Custom Spark Image**: Built with hadoop-aws and AWS SDK for S3A support
2. **Auto-initialization**: MinIO buckets, policies, and users created on startup
3. **Service Users**: Spark, Trino, and MLflow use dedicated service accounts (not root)
4. **Least Privilege**: Each service has minimal required permissions
5. **Hadoop Config**: Custom core-site.xml to fix duration-string parsing issues
6. **Trino Iceberg Catalog**: Pre-configured with PostgreSQL JDBC catalog (no REST catalog/Gravitino needed)
7. **Iceberg Metadata**: Catalog schema tables automatically created in PostgreSQL on first startup

## Files Structure

```
docker/
â”œâ”€â”€ docker-compose.yml          # Main orchestration file
â”œâ”€â”€ .env                        # Environment variables (DO NOT COMMIT)
â”œâ”€â”€ .env.example               # Template for .env
â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ postgres-init.sh       # Auto-creates databases
â”‚   â””â”€â”€ postgres-init.sql      # SQL initialization
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ Dockerfile             # Custom Spark image with S3A
â”‚   â””â”€â”€ core-site.xml          # Hadoop configuration
â”œâ”€â”€ trino/
â”‚   â””â”€â”€ catalog/
â”‚       â”œâ”€â”€ iceberg.properties         # Trino-Iceberg catalog configuration
â”‚       â””â”€â”€ iceberg.properties.template # Template with env vars
â”œâ”€â”€ gravitino/
â”‚   â””â”€â”€ Dockerfile             # Iceberg REST catalog image
â””â”€â”€ scripts/
    â””â”€â”€ health-check.sh        # Comprehensive health check script
```

## For More Details

See [SETUP_GUIDE.md](SETUP_GUIDE.md) for:
- Detailed architecture
- Configuration options
- Advanced troubleshooting
- Production considerations
- Backup and recovery

## Security Notes (âš ï¸ Important for Production)

**Current configuration uses default credentials for LOCAL DEVELOPMENT only.**

For production:
1. Copy `.env.example` to `.env`
2. Change all passwords and access keys
3. Use secrets management (e.g., Docker secrets, Vault)
4. Enable TLS/SSL for all services
5. Configure firewall rules
6. Use separate service accounts for each component

## Support

If verification fails:
1. Check the script output for specific failures
2. View service logs: `docker compose logs <service>`
3. Ensure no port conflicts
4. Verify Docker has sufficient resources (8GB+ RAM recommended)
5. Check the SETUP_GUIDE.md troubleshooting section

---

**Happy Data Engineering!** ğŸ‰
