name: dlhpv

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"

services:
  minio:
    image: ${MINIO_IMAGE}
    container_name: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":${MINIO_CONSOLE_PORT}"
    ports:
      - "${MINIO_API_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:${MINIO_CONSOLE_PORT}"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9000/minio/health/ready >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
    restart: unless-stopped
    logging: *default-logging
    networks: [data-net]
    profiles: [core]

  mc:
    image: ${MC_IMAGE}
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ../infra/minio/policies:/policies:ro
    environment:
      MINIO_ENDPOINT: ${MINIO_ENDPOINT}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      S3_WAREHOUSE_BUCKET: ${S3_WAREHOUSE_BUCKET}
      S3_MLFLOW_BUCKET: ${S3_MLFLOW_BUCKET}
      SPARK_SVC_ACCESS_KEY: ${SPARK_SVC_ACCESS_KEY}
      SPARK_SVC_SECRET_KEY: ${SPARK_SVC_SECRET_KEY}
      TRINO_SVC_ACCESS_KEY: ${TRINO_SVC_ACCESS_KEY}
      TRINO_SVC_SECRET_KEY: ${TRINO_SVC_SECRET_KEY}
      MLFLOW_SVC_ACCESS_KEY: ${MLFLOW_SVC_ACCESS_KEY}
      MLFLOW_SVC_SECRET_KEY: ${MLFLOW_SVC_SECRET_KEY}
    entrypoint: >
      /bin/sh -c "
      set -e;
      mc alias set local ${MINIO_ENDPOINT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      echo 'Creating buckets...';
      mc mb -p local/${S3_WAREHOUSE_BUCKET} || true;
      mc mb -p local/${S3_MLFLOW_BUCKET} || true;
      echo 'Setting bucket access to private...';
      mc anonymous set private local/${S3_WAREHOUSE_BUCKET} || true;
      mc anonymous set private local/${S3_MLFLOW_BUCKET} || true;
      echo 'Enabling versioning...';
      mc version enable local/${S3_WAREHOUSE_BUCKET} || true;
      mc version enable local/${S3_MLFLOW_BUCKET} || true;
      echo 'Creating policies...';
      mc admin policy create local lakehouse-rw /policies/lakehouse-rw.json || true;
      mc admin policy create local mlflow-rw /policies/mlflow-rw.json || true;
      echo 'Creating service users...';
      mc admin user add local ${SPARK_SVC_ACCESS_KEY} ${SPARK_SVC_SECRET_KEY} || true;
      mc admin user add local ${TRINO_SVC_ACCESS_KEY} ${TRINO_SVC_SECRET_KEY} || true;
      mc admin user add local ${MLFLOW_SVC_ACCESS_KEY} ${MLFLOW_SVC_SECRET_KEY} || true;
      echo 'Attaching policies to users...';
      mc admin policy attach local lakehouse-rw --user ${SPARK_SVC_ACCESS_KEY};
      mc admin policy attach local lakehouse-rw --user ${TRINO_SVC_ACCESS_KEY};
      mc admin policy attach local mlflow-rw --user ${MLFLOW_SVC_ACCESS_KEY};
      echo 'MinIO setup complete!';
      exit 0"
    logging: *default-logging
    networks: [data-net]
    profiles: [core]

  postgres:
    image: ${POSTGRES_IMAGE}
    container_name: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres:/docker-entrypoint-initdb.d:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 12
    restart: unless-stopped
    logging: *default-logging
    networks: [data-net]
    profiles: [core]

  pgadmin:
    image: ${PGADMIN_IMAGE}
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - pgadmin-data:/var/lib/pgadmin
    ports:
      - "${PGADMIN_PORT}:80"
    restart: unless-stopped
    logging: *default-logging
    networks: [data-net]
    profiles: [core]

  # iceberg-rest service no longer needed - using JDBC catalog directly with PostgreSQL
  # Removed Gravitino to eliminate Hadoop compatibility issues
  # Iceberg metadata is now stored directly in PostgreSQL (iceberg_catalog database)
  
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    image: dlhpv-spark-s3a:local
    container_name: spark-master
    command: ["/opt/spark/sbin/start-master.sh", "-p", "7077", "--host", "spark-master"]
    user: root
    environment:
      SPARK_NO_DAEMONIZE: "true"
      AWS_ACCESS_KEY_ID: ${MLFLOW_SVC_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MLFLOW_SVC_SECRET_KEY}
      AWS_REGION: ${S3_REGION}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      PYTHONPATH: /opt/workdir/src:$PYTHONPATH
      OPEN_NEM_PRIMARY: ${OPEN_NEM_PRIMARY}
      OPEN_NEM_SECONDARY: ${OPEN_NEM_SECONDARY}
      OPENELECTRICITY_API_KEY: ${OPEN_NEM_PRIMARY}
      MLFLOW_TRACKING_URI: http://mlflow:${MLFLOW_PORT}
      SPARK_SUBMIT_OPTIONS: "--conf spark.mlflow.tracking.uri=http://mlflow:${MLFLOW_PORT}"
    volumes:
      - ../:/opt/workdir:ro
      - ./spark/core-site.xml:/opt/spark/conf/core-site.xml:ro
    ports:
      - "${SPARK_UI_PORT}:8080"
      - "7077:7077" 
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080 >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 12
    restart: unless-stopped
    logging: *default-logging
    networks: [data-net]
    profiles: [core]

  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    image: dlhpv-spark-s3a:local
    container_name: spark-worker
    command: ["/opt/spark/sbin/start-worker.sh", "spark://spark-master:7077"]
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_NO_DAEMONIZE: "true"
      SPARK_WORKER_MEMORY: 4G
      SPARK_WORKER_CORES: 2
      AWS_ACCESS_KEY_ID: ${MLFLOW_SVC_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MLFLOW_SVC_SECRET_KEY}
      AWS_REGION: ${S3_REGION}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      MLFLOW_TRACKING_URI: http://mlflow:${MLFLOW_PORT}
      SPARK_SUBMIT_OPTIONS: "--conf spark.mlflow.tracking.uri=http://mlflow:${MLFLOW_PORT}"
    volumes:
      - ../:/opt/workdir:ro
      - ./spark/core-site.xml:/opt/spark/conf/core-site.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8081 >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 12
    deploy:
      resources:
        limits:
          memory: 4g
    restart: unless-stopped
    logging: *default-logging
    networks: [data-net]
    profiles: [core]

  trino:
    image: ${TRINO_IMAGE}
    container_name: trino
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      mc:
        condition: service_completed_successfully
    environment:
      MINIO_ENDPOINT: ${MINIO_ENDPOINT}
      S3_REGION: ${S3_REGION}
      S3_WAREHOUSE_BUCKET: ${S3_WAREHOUSE_BUCKET}
      TRINO_SVC_ACCESS_KEY: ${TRINO_SVC_ACCESS_KEY}
      TRINO_SVC_SECRET_KEY: ${TRINO_SVC_SECRET_KEY}
    volumes:
      - ./trino/catalog:/etc/trino/catalog:ro
    ports:
      - "${TRINO_PORT}:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/v1/info >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
    restart: unless-stopped
    logging: *default-logging
    networks: [data-net]
    profiles: [core]

  mlflow:
    image: ${MLFLOW_BASE_IMAGE}
    container_name: mlflow
    depends_on:
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    working_dir: /app
    command: >
      bash -lc "
      pip install --no-cache-dir mlflow psycopg2-binary boto3 || true;
      mlflow server \
      --backend-store-uri postgresql://${MLFLOW_USER}:${MLFLOW_PASSWORD}@postgres:5432/${MLFLOW_DB} \
      --default-artifact-root s3://${S3_MLFLOW_BUCKET}/artifacts \
      --host 0.0.0.0 --port ${MLFLOW_PORT}
      "
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${MLFLOW_SVC_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MLFLOW_SVC_SECRET_KEY}
      AWS_REGION: ${S3_REGION}
      MLFLOW_TRACKING_URI: "http://mlflow:${MLFLOW_PORT}"
      MLFLOW_SERVER_ALLOWED_HOSTS: "mlflow,mlflow:${MLFLOW_PORT},spark-master,spark-master:${MLFLOW_PORT},spark-master.dlhpv_data-net,spark-master.dlhpv_data-net:${MLFLOW_PORT},localhost,localhost:${MLFLOW_PORT},127.0.0.1,127.0.0.1:${MLFLOW_PORT}"
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/')"]
      interval: 10s
      timeout: 5s
      retries: 12
    restart: unless-stopped
    logging: *default-logging
    networks: [data-net]
    profiles: [ml]

  #prefect:
  #  image: ${PREFECT_IMAGE}
  #  container_name: prefect
  #  depends_on:
  #    postgres:
  #      condition: service_healthy
  #  working_dir: /repo
  #  command: >
  #    bash -lc "
  #    pip install --no-cache-dir asyncpg || true;
  #    prefect server start --host 0.0.0.0
  #    "
  #  environment:
  #    # Use Postgres for the Prefect API database instead of the default SQLite
  #    PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/prefect
  #    PREFECT_API_URL: ${PREFECT_API_URL}
  #  ports:
  #    - "${PREFECT_PORT}:4200"
  #  healthcheck:
  #    test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4200/api/health')"]
  #    interval: 10s
  #    timeout: 5s
  #    retries: 12
  #  restart: unless-stopped
  #  logging: *default-logging
  #  networks: [data-net]
  #  profiles: [orchestrate]

  #prefect-agent:
  #  image: ${PREFECT_IMAGE}
  #  container_name: prefect-agent
  #  depends_on:
  #    prefect:
  #      condition: service_healthy
  #  working_dir: /repo
  #  volumes:
  #    - ../:/repo
  #    - /var/run/docker.sock:/var/run/docker.sock
  #  environment:
  #    PREFECT_API_URL: ${PREFECT_API_URL}
  #    PREFECT_WORK_POOL: default
  #    PV_LAKEHOUSE_ROOT: /repo
  #    DOCKER_COMPOSE_FILE: /repo/docker/docker-compose.yml
  #    DOCKER_COMPOSE_CMD: "docker compose"
  #  command: >
  #    bash -lc '
  #    PREFECT_POOL=$${PREFECT_WORK_POOL:-default};
  #    if ! command -v docker >/dev/null 2>&1 || ! docker compose version >/dev/null 2>&1; then
  #      apt-get update && apt-get install -y ca-certificates curl gnupg && 
  #      install -m 0755 -d /etc/apt/keyrings && 
  #      curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg && 
  #      chmod a+r /etc/apt/keyrings/docker.gpg && 
  #      echo "deb [arch=$$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian bookworm stable" > /etc/apt/sources.list.d/docker.list && 
  #      apt-get update && apt-get install -y docker-ce-cli docker-compose-plugin && 
  #      rm -rf /var/lib/apt/lists/*;
  #    fi;
  #    prefect work-pool create $$PREFECT_POOL --type process || true;
  #    prefect worker start --pool $$PREFECT_POOL
  #    '
  #  restart: unless-stopped
  #  logging: *default-logging
  #  networks: [data-net]
  #  profiles: [orchestrate]

networks:
  data-net:

volumes:
  pgadmin-data:
  minio-data:
  postgres-data: