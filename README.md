<p align="center">
  <img src="doc/assets/logo.png" alt="PV Lakehouse Logo" width="120" height="120">
  <h1 align="center">ğŸ  PV Lakehouse</h1>
  <p align="center">
    <strong>A production-ready data lakehouse platform for solar energy analytics</strong>
  </p>
  <p align="center">
    <a href="#-quick-start">Quick Start</a> â€¢
    <a href="#-features">Features</a> â€¢
    <a href="#-architecture">Architecture</a> â€¢
    <a href="#-documentation">Documentation</a> â€¢
    <a href="#-contributing">Contributing</a>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/python-3.11+-blue.svg" alt="Python">
    <img src="https://img.shields.io/badge/spark-3.5-orange.svg" alt="Spark">
    <img src="https://img.shields.io/badge/iceberg-1.5-green.svg" alt="Iceberg">
    <img src="https://img.shields.io/badge/docker-ready-blue.svg" alt="Docker">
    <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License">
  </p>
</p>

---

## ğŸ“‹ Overview

**PV Lakehouse** is a complete, open-source data lakehouse solution designed for solar energy analytics. Built with modern data engineering best practices, it provides end-to-end capabilities from raw data ingestion to ML-ready feature stores.

### âœ¨ Key Highlights

| Feature | Description |
|---------|-------------|
| ğŸ—ï¸ **Medallion Architecture** | Bronze â†’ Silver â†’ Gold data layers with clear data contracts |
| ğŸ³ **Docker-native** | One-command deployment with Docker Compose profiles |
| ğŸ”Œ **Open Standards** | Apache Iceberg table format for interoperability |
| ğŸ“Š **SQL-first** | Query data directly with Trino's ANSI SQL engine |
| ğŸ¤– **ML-ready** | Integrated MLflow for experiment tracking and model management |
| âš¡ **High Performance** | Optimized Spark configurations for batch processing |
| ğŸ”„ **ELT Pattern** | Extract-Load-Transform for data lineage & reproducibility |

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology | Version | Purpose |
|-------|------------|---------|---------|
| **Storage** | MinIO | Latest | S3-compatible object storage |
| **Table Format** | Apache Iceberg | 1.5 | ACID transactions, schema evolution |
| **Processing** | Apache Spark | 3.5 | Distributed batch processing |
| **Query Engine** | Trino | Latest | Interactive SQL analytics |
| **Orchestration** | Prefect | 2.x | Workflow automation |
| **ML Tracking** | MLflow | 2.4 | Experiment tracking & model registry |
| **Catalog** | PostgreSQL | 15 | Iceberg metadata store |

---

## ğŸ›ï¸ Architecture

### Medallion Data Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MEDALLION ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      ğŸ¥‰ BRONZE        â”‚      ğŸ¥ˆ SILVER        â”‚         ğŸ¥‡ GOLD             â”‚
â”‚      (Raw Data)       â”‚     (Cleaned)         â”‚      (Analytics)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Schema-on-read      â”‚ â€¢ Schema enforcement  â”‚ â€¢ Star schema               â”‚
â”‚ â€¢ Full fidelity       â”‚ â€¢ Deduplication       â”‚ â€¢ Pre-aggregated            â”‚
â”‚ â€¢ Append-only         â”‚ â€¢ Data validation     â”‚ â€¢ Business metrics          â”‚
â”‚ â€¢ Audit trail         â”‚ â€¢ Type casting        â”‚ â€¢ ML features               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ raw_facilities        â”‚ clean_facility_master â”‚ dim_facility                â”‚
â”‚ raw_facility_         â”‚ clean_hourly_energy   â”‚ dim_date                    â”‚
â”‚   timeseries          â”‚ clean_hourly_weather  â”‚ dim_time                    â”‚
â”‚ raw_facility_weather  â”‚ clean_hourly_         â”‚ dim_aqi_category            â”‚
â”‚ raw_facility_         â”‚   air_quality         â”‚ fact_solar_environmental    â”‚
â”‚   air_quality         â”‚                       â”‚                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Infrastructure Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INFRASTRUCTURE                                    â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚   MinIO     â”‚  â”‚ PostgreSQL  â”‚  â”‚    Trino    â”‚  â”‚   Spark     â”‚      â”‚
â”‚   â”‚   (S3)      â”‚  â”‚  (Catalog)  â”‚  â”‚   (Query)   â”‚  â”‚  (Process)  â”‚      â”‚
â”‚   â”‚  :9000/9001 â”‚  â”‚    :5432    â”‚  â”‚    :8081    â”‚  â”‚    :4040    â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚   MLflow    â”‚  â”‚   Prefect   â”‚  â”‚   pgAdmin   â”‚                       â”‚
â”‚   â”‚   (ML Ops)  â”‚  â”‚   (Orch)    â”‚  â”‚    (UI)     â”‚                       â”‚
â”‚   â”‚    :5000    â”‚  â”‚    :4200    â”‚  â”‚    :5050    â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Version |
|-------------|---------|
| Docker | 20.10+ |
| Docker Compose | 2.0+ |
| Python | 3.11+ (for development) |
| Git | Latest |

### 1. Clone & Configure

```bash
# Clone the repository
git clone https://github.com/yourusername/dlh-pv.git
cd dlh-pv

# Setup environment configuration
cp docker/.env.example docker/.env

# Create symlink (optional, for convenience)
ln -sf docker/.env .env
```

### 2. Start Services

```bash
cd docker

# Start core services
docker compose --profile core up -d

# Verify health
./scripts/health-check.sh
```

### 3. Run Your First Pipeline

```bash
# Load facility metadata
docker compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client --driver-memory 3g --executor-memory 4g \
  /opt/workdir/src/pv_lakehouse/etl/bronze/load_facilities.py

# Query the data
docker exec -it trino trino --execute \
  "SELECT * FROM iceberg.bronze.raw_facilities LIMIT 5"
```

### 4. Access Web Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **MinIO Console** | http://localhost:9001 | `pvlakehouse` / `pvlakehouse` |
| **Spark Master UI** | http://localhost:4040 | â€” |
| **Trino UI** | http://localhost:8081 | â€” |
| **MLflow UI** | http://localhost:5000 | â€” |
| **pgAdmin** | http://localhost:5050 | `admin@example.com` / `pvlakehouse` |

---

## ğŸ“ Project Structure

```
dlh-pv/
â”œâ”€â”€ ğŸ“‚ docker/                    # Docker Compose configuration
â”‚   â”œâ”€â”€ docker-compose.yml        # Service definitions
â”‚   â”œâ”€â”€ .env.example              # Environment template
â”‚   â”œâ”€â”€ README-SETUP.md           # Docker setup guide
â”‚   â”œâ”€â”€ postgres/                 # PostgreSQL init scripts
â”‚   â”œâ”€â”€ spark/                    # Spark Dockerfile & config
â”‚   â””â”€â”€ trino/                    # Trino catalog config
â”‚
â”œâ”€â”€ ğŸ“‚ src/pv_lakehouse/          # Main Python package
â”‚   â”œâ”€â”€ config/                   # Configuration management
â”‚   â”‚   â”œâ”€â”€ settings.py           # Pydantic settings
â”‚   â”‚   â””â”€â”€ spark_config.yaml     # Spark configuration
â”‚   â”œâ”€â”€ etl/                      # ETL modules
â”‚   â”‚   â”œâ”€â”€ bronze/               # Raw data ingestion
â”‚   â”‚   â”œâ”€â”€ silver/               # Data transformation
â”‚   â”‚   â”œâ”€â”€ gold/                 # Analytics layer
â”‚   â”‚   â”œâ”€â”€ clients/              # API clients
â”‚   â”‚   â”œâ”€â”€ utils/                # Shared utilities
â”‚   â”‚   â””â”€â”€ scripts/              # Helper scripts
â”‚   â””â”€â”€ ml_pipeline/              # ML training pipelines
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                     # Test suite
â”‚   â”œâ”€â”€ config/                   # Config tests
â”‚   â”œâ”€â”€ etl/                      # ETL tests
â”‚   â””â”€â”€ conftest.py               # Pytest fixtures
â”‚
â”œâ”€â”€ ğŸ“‚ doc/                       # Documentation
â”‚   â”œâ”€â”€ schema/                   # Schema definitions
â”‚   â””â”€â”€ power-bi/                 # BI integration guides
â”‚
â”œâ”€â”€ ğŸ“‚ dashboard/                 # Power BI dashboards
â”œâ”€â”€ ğŸ“‚ config/                    # ML configuration
â”œâ”€â”€ pyproject.toml                # Python project config
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸ“š Documentation

### Guides

| Document | Description |
|----------|-------------|
| [Docker Setup](docker/README-SETUP.md) | Complete Docker deployment guide |
| [ETL Operations](src/pv_lakehouse/etl/scripts/CHEATSHEET_GUIDE.md) | ETL pipeline operations |
| [Gold Layer Design](doc/schema/GOLD_LAYER_DESIGN.md) | Analytics schema design |
| [Trino Connection](doc/power-bi/TRINO_CONNECTION_GUIDE.md) | BI tool integration |

### Data Layers

| Layer | Table | Description |
|-------|-------|-------------|
| **Bronze** | `raw_facilities` | Solar facility metadata |
| | `raw_facility_timeseries` | Energy generation data |
| | `raw_facility_weather` | Weather observations |
| | `raw_facility_air_quality` | Air quality metrics |
| **Silver** | `clean_facility_master` | Validated facility data |
| | `clean_hourly_energy` | Hourly energy aggregates |
| | `clean_hourly_weather` | Hourly weather data |
| | `clean_hourly_air_quality` | Hourly air quality |
| **Gold** | `dim_*` | Dimension tables |
| | `fact_solar_environmental` | Main fact table |

---

## ğŸ§ª Development

### Local Setup

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or: .venv\Scripts\activate  # Windows

# Install dependencies
pip install -e .
pip install -r requirements.txt
```

### Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src/pv_lakehouse --cov-report=html

# Run specific test file
pytest tests/config/test_settings.py -v
```

### Code Quality

```bash
# Format code
ruff format src/ tests/

# Lint
ruff check src/ tests/

# Type checking
mypy src/pv_lakehouse/
```

---

## ğŸ”§ Configuration

### Environment Variables

All configuration is managed via `docker/.env`:

| Category | Key Variables |
|----------|---------------|
| **Credentials** | `PV_USER`, `PV_PASSWORD` |
| **PostgreSQL** | `POSTGRES_HOST`, `POSTGRES_PORT` |
| **MinIO** | `MINIO_ENDPOINT`, `S3_WAREHOUSE_BUCKET` |
| **Spark** | `SPARK_WORKER_MEMORY`, `SPARK_EXECUTOR_MEMORY` |
| **API Keys** | `OPENELECTRICITY_API_KEY` |

### Spark Tuning

Adjust in `.env` based on your system:

```env
# For 16GB RAM system
SPARK_WORKER_MEMORY=6G
SPARK_EXECUTOR_MEMORY=4g
SPARK_DRIVER_MEMORY=3g
SPARK_SHUFFLE_PARTITIONS=32
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Coding Standards

- Follow [PEP 8](https://pep8.org/) style guide
- Use type hints for all functions
- Write docstrings (Google style)
- Maintain test coverage > 80%
- Use `ruff` for formatting

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Apache Iceberg](https://iceberg.apache.org/) - Table format
- [Apache Spark](https://spark.apache.org/) - Processing engine
- [Trino](https://trino.io/) - Query engine
- [MinIO](https://min.io/) - Object storage
- [OpenElectricity](https://openelectricity.org.au/) - Data source

---

<p align="center">
  <sub>Built with â¤ï¸ for the solar energy community</sub>
</p>
