<p align="center">
  <h1 align="center">ğŸ  PV Lakehouse</h1>
  <p align="center">
    <strong>A production-ready data lakehouse platform for building modern ETL pipelines</strong>
  </p>
  <p align="center">
    <a href="#-quick-start">Quick Start</a> â€¢
    <a href="#-features">Features</a> â€¢
    <a href="#-architecture">Architecture</a> â€¢
    <a href="#-documentation">Documentation</a>
  </p>
</p>

---

## ğŸ“‹ Overview

**PV Lakehouse** is a complete data lakehouse solution designed to run on your laptop or a single VM. Built entirely with open-source components, it provides a robust foundation for evolving data from raw ingestion through normalization to curated analytics.

### âœ¨ Key Highlights

- ğŸ—ï¸ **Medallion Architecture** â€” Bronze â†’ Silver â†’ Gold data layers with clear conventions
- ğŸ³ **Docker-native** â€” One-command deployment with Docker Compose profiles
- ğŸ”Œ **Open Standards** â€” Apache Iceberg table format for interoperability
- ğŸ“Š **SQL-first** â€” Query data directly with Trino's ANSI SQL engine
- ğŸ¤– **ML-ready** â€” Integrated MLflow for experiment tracking and model management
- ğŸ”„ **Workflow Orchestration** â€” Prefect-powered ETL pipelines

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Object Storage** | MinIO | S3-compatible storage for data lake |
| **Table Format** | Apache Iceberg v2 | Open table format with ACID transactions |
| **Batch Processing** | Apache Spark 3.5 | Distributed data processing engine |
| **Query Engine** | Trino | Fast SQL analytics over lakehouse |
| **Orchestration** | Prefect | Modern workflow orchestration |
| **ML Tracking** | MLflow 2.4 | Experiment tracking & model registry |
| **Metadata Store** | PostgreSQL | Iceberg catalog & application metadata |
| **Admin UI** | pgAdmin | Database management interface |

---

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              MEDALLION LAYERS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    ğŸ¥‰ BRONZE      â”‚    ğŸ¥ˆ SILVER      â”‚         ğŸ¥‡ GOLD               â”‚
â”‚    (Raw Data)     â”‚   (Normalized)    â”‚    (Curated/Analytics)        â”‚
â”‚                   â”‚                   â”‚                               â”‚
â”‚  â€¢ Raw ingestion  â”‚  â€¢ Cleaned data   â”‚  â€¢ Aggregations               â”‚
â”‚  â€¢ Schema-on-read â”‚  â€¢ Validated      â”‚  â€¢ Business metrics           â”‚
â”‚  â€¢ Full fidelity  â”‚  â€¢ Deduplicated   â”‚  â€¢ ML feature tables          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INFRASTRUCTURE                                â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  MinIO  â”‚  â”‚  Spark  â”‚  â”‚  Trino  â”‚  â”‚ MLflow  â”‚  â”‚Postgres â”‚     â”‚
â”‚   â”‚   S3    â”‚  â”‚ Cluster â”‚  â”‚  Query  â”‚  â”‚   ML    â”‚  â”‚ Catalog â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Repository Structure

```
dlh-pv/
â”œâ”€â”€ doc/                         # ğŸ“š Comprehensive documentation
â”‚   â”œâ”€â”€ bronze-silver/           # Data layer specifications
â”‚   â”œâ”€â”€ schema/                  # Schema definitions
â”‚   â””â”€â”€ power-bi/                # BI integration guides
â”‚
â”œâ”€â”€ docker/                      # ğŸ³ Docker Compose services
â”‚   â”œâ”€â”€ docker-compose.yml       # Main compose file with profiles
â”‚   â”œâ”€â”€ postgres/                # PostgreSQL initialization scripts
â”‚   â”œâ”€â”€ spark/                   # Spark Dockerfile & configuration
â”‚   â”œâ”€â”€ trino/                   # Trino catalog configuration
â”‚   â””â”€â”€ scripts/                 # Utility & health-check scripts
â”‚
â”œâ”€â”€ infra/                       # ğŸ—„ï¸ Infrastructure configuration
â”‚   â””â”€â”€ minio/policies/          # MinIO bucket policies
â”‚
â”œâ”€â”€ src/pv_lakehouse/            # ğŸ Python package
â”‚   â”œâ”€â”€ etl/                     # ETL modules (bronze, silver, gold)
â”‚   â”‚   â”œâ”€â”€ bronze/              # Raw data ingestion
â”‚   â”‚   â”œâ”€â”€ silver/              # Data transformation
â”‚   â”‚   â”œâ”€â”€ gold/                # Analytics & aggregations
â”‚   â”‚   â”œâ”€â”€ clients/             # External API clients
â”‚   â”‚   â””â”€â”€ notebooks/           # Jupyter notebooks
â”‚   â”œâ”€â”€ ml_pipeline/             # Machine learning pipelines
â”‚   â””â”€â”€ mlflow/                  # MLflow integration
â”‚
â”œâ”€â”€ pyproject.toml               # Python project configuration
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Version |
|-------------|---------|
| Docker | 20.10+ |
| Docker Compose | 2.0+ |
| Python | 3.11+ (for development) |
| Git | Latest |

### 1. Clone & Configure

```bash
# Clone the repository
git clone https://github.com/xuanquangIT/dlh-pv.git
cd dlh-pv

# Copy environment template
cp docker/.env.example docker/.env
```

### 2. Start Services

```bash
cd docker

# Start core services (MinIO, PostgreSQL, Trino, Spark)
docker compose --profile core up -d

# Optionally, start ML services (MLflow)
docker compose --profile ml up -d

# Verify all services are healthy
./scripts/health-check.sh
```

### 3. Access Web Interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **MinIO Console** | [localhost:9001](http://localhost:9001) | `pvlakehouse` / `pvlakehouse` |
| **Spark Master UI** | [localhost:8080](http://localhost:8080) | â€” |
| **Trino UI** | [localhost:8081](http://localhost:8081) | â€” |
| **MLflow UI** | [localhost:5000](http://localhost:5000) | â€” |
| **pgAdmin** | [localhost:5050](http://localhost:5050) | `admin@admin.com` / `pvlakehouse` |

### 4. Run Your First Query

```bash
# Connect to Trino CLI
docker exec -it trino trino --catalog iceberg --schema bronze

# Show available tables
trino:bronze> SHOW TABLES;

# Query sample data
trino:bronze> SELECT * FROM your_table LIMIT 10;
```

---

## ğŸ“š Documentation

Comprehensive documentation is available in the [`doc/`](doc/) directory:

### Data Layer Guides

| Document | Description |
|----------|-------------|
| [Bronze Layer](doc/bronze-silver/BRONZE_LAYER.md) | Raw data ingestion specifications |
| [Silver Layer](doc/bronze-silver/SILVER_LAYER.md) | Data transformation & validation |
| [Silver Validation Rules](doc/bronze-silver/SILVER_VALIDATION_RULES.md) | Data quality checks |
| [ETL Operations Guide](doc/bronze-silver/ETL_OPERATIONS_GUIDE.md) | Running ETL pipelines |

### Analysis & Troubleshooting

| Document | Description |
|----------|-------------|
| [Bronze-Silver Analysis](doc/bronze-silver/BRONZE_SILVER_ANALYSIS_README.md) | Data flow analysis |
| [Anomalies & Filters](doc/bronze-silver/ANOMALIES_AND_SILVER_FILTERS.md) | Data quality patterns |
| [Timezone Analysis](doc/bronze-silver/TIMEZONE_AND_RECORD_COUNT_ANALYSIS.md) | Temporal data handling |

---

## ğŸ§ª Development

### Setting Up Local Environment

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -e .
pip install -r requirements.txt
```

### Running Tests

```bash
# Run pytest suite
pytest tests/

# Run specific test
pytest tests/test_bronze_tables_complete.py -v
```

### Project Configuration

The project uses modern Python tooling:

- **Build System**: setuptools with `pyproject.toml`
- **Linting**: Ruff (line-length: 100, Python 3.11+)
- **Testing**: pytest

---

## ğŸ”§ Docker Compose Profiles

The platform uses Docker Compose profiles for flexible deployment:

| Profile | Services | Use Case |
|---------|----------|----------|
| `core` | MinIO, PostgreSQL, Spark, Trino, pgAdmin | Data engineering workloads |
| `ml` | MLflow | Machine learning workflows |

```bash
# Start specific profile
docker compose --profile core up -d

# Start multiple profiles
docker compose --profile core --profile ml up -d

# Stop all services
docker compose --profile core --profile ml down
```

---

## ğŸ“Š Use Cases

PV Lakehouse is designed for:

- ğŸ”¬ **Data Engineering Learning** â€” Hands-on experience with modern lakehouse architecture
- ğŸ§ª **Prototype Development** â€” Quickly validate ETL pipelines before production
- ğŸ“ˆ **Analytics Workloads** â€” SQL-based analysis with Trino
- ğŸ¤– **ML Experiments** â€” Track experiments and models with MLflow
- ğŸ  **Local Development** â€” Full lakehouse stack on a single machine

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

---

## ğŸ“„ License

This project is open source. See the repository for license details.

---

<p align="center">
  <sub>Built with â¤ï¸ using open-source technologies</sub>
</p>
