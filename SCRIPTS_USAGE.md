# H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Scripts

## üöÄ Quick Start

### 1. Train & Load to Gold (to√†n b·ªô data)
```bash
./quick_regression.sh
```

### 2. Ki·ªÉm tra k·∫øt qu·∫£
```bash
./check_regression_results.sh
```

---

## üìä Chi Ti·∫øt Pipeline

### `quick_regression.sh` th·ª±c hi·ªán:
1. **Train Model**: RandomForestRegressor v·ªõi **ALL data** (--limit 0)
2. **Save to Gold**: Predictions ƒë∆∞·ª£c l∆∞u tr·ª±c ti·∫øp v√†o Gold layer
3. **Query Results**: Hi·ªÉn th·ªã metrics t·ªïng quan

**Input:** `lh.silver.clean_hourly_energy` + `clean_hourly_weather`  
**Output:** `lh.gold.fact_solar_forecast_regression`  
**Write Mode:** OVERWRITE (x√≥a predictions c≈©, ch·ªâ gi·ªØ k·∫øt qu·∫£ train m·ªõi nh·∫•t)  
**Split Method:** Deterministic temporal split (95% train / 5% test)  
**Th·ªùi gian:** ~5-10 ph√∫t

---

### `check_regression_results.sh` hi·ªÉn th·ªã:
- Performance metrics (MAPE, R¬≤, accuracy rate)
- Top 10 best predictions
- Performance by facility

---

## ‚ùì FAQ

### Q: Script c√≥ d√πng to√†n b·ªô data kh√¥ng?
**A:** ‚úÖ C√ì - `--limit 0` = kh√¥ng gi·ªõi h·∫°n (~161K rows)

**Ch·ª©ng minh:**
```bash
# quick_regression.sh d√≤ng 14:
python3 train_regression_model.py --limit 0

# Output khi ch·∫°y:
[TRAIN] Using ALL available data (no limit)
[TRAIN] Loaded 161020 samples
[SPLIT] Training: 152769 samples (94.9%)
[SPLIT] Test: 8251 samples (5.1%)
```

**L∆∞u √Ω:** Split l√† **DETERMINISTIC** - m·ªói l·∫ßn train s·∫Ω c√≥ C√ôNG test set.

---

### Q: T·∫°i sao m·ªói l·∫ßn train c√≥ s·ªë predictions kh√°c nhau?
**A:** ‚ùå **KH√îNG N√äN X·∫¢Y RA** - ƒê√£ fix b·∫±ng deterministic split!

**Tr∆∞·ªõc khi fix:**
- D√πng `approxQuantile()` ‚Üí split point dao ƒë·ªông ¬±1%
- L·∫ßn 1: 8,700 test samples
- L·∫ßn 2: 8,880 test samples  
- L·∫ßn 3: 8,730 test samples

**Sau khi fix:**
- D√πng exact row number split
- **M·ªåI L·∫¶N TRAIN GI·ªêNG H·ªÜT:** ~8,251 test samples
- ƒê·∫£m b·∫£o reproducibility cho research

---

### Q: Model parameters hi·ªán t·∫°i?
```python
RandomForestRegressor(
    numTrees=25,              # Gi·∫£m t·ª´ 30
    maxDepth=10,              # Gi·∫£m t·ª´ 12
    minInstancesPerNode=80,   # TƒÉng t·ª´ 50
    subsamplingRate=0.7,      # Gi·∫£m t·ª´ 0.8
    featureSubsetStrategy="sqrt"
)
```
**M·ª•c ƒë√≠ch:** Anti-overfitting

---

### Q: Expected performance?
- **R¬≤ Score:** 84-88%
- **MAPE:** <50%
- **Accuracy (‚â§10% MAPE):** >45%

---

## üîß Queries Th·ªß C√¥ng

### Xem total predictions:
```bash
docker compose -f docker/docker-compose.yml exec trino trino \
  --server http://trino:8080 --catalog iceberg --schema gold \
  --execute "SELECT COUNT(*) FROM fact_solar_forecast_regression"
```

### Xem 20 predictions m·ªõi nh·∫•t:
```bash
docker compose -f docker/docker-compose.yml exec trino trino \
  --server http://trino:8080 --catalog iceberg --schema gold \
  --execute "
    SELECT * FROM fact_solar_forecast_regression 
    ORDER BY forecast_timestamp DESC LIMIT 20
  "
```

---

## üìç Resources

- **MLflow UI:** http://localhost:5002
- **Trino Console:** http://localhost:8080
- **Training Script:** `src/pv_lakehouse/mlflow/train_regression_model.py`
- **Gold Loader:** `src/pv_lakehouse/etl/gold/fact_solar_forecast_regression.py`
